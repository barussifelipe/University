# Compute the proportion of variance explained by each component.
# How many PCs are needed to explain at least 90% of the variance?
var_explained_iris = pca_iris$sdev**2/sum(pca_iris$sdev**2)
var_explained_iris
# üß™ EXERCISE 4 ‚Äî Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
# üß™ EXERCISE 5 ‚Äî Standardization test ------------------------------------
# Run PCA on unscaled iris data. Compare results with the scaled version.
# Why is scaling important?
############################################################
# END OF LESSON
############################################################
fviz_pca_var(pca_iris,
col.var = "contrib",
gradient.cols = c("white", "blue", "red"),
repel = TRUE)
# üß™ EXERCISE 4 ‚Äî Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
fviz_pca_ind(pca_iris, repel = TRUE, col.var = pca_df$Species)
# üß™ EXERCISE 4 ‚Äî Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
fviz_pca_ind(pca_iris, repel = TRUE, col.ind = pca_df$Species)
## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa
fviz_pca_var(pca_iris,
col.var = "contrib",
gradient.cols = c("white", "blue", "red"),
repel = TRUE)
# üß™ EXERCISE 4 ‚Äî Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
fviz_pca_var(pca_iris, repel = TRUE, col.var = pca_df$Species)
pca_df$Species
# üß™ EXERCISE 4 ‚Äî Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
fviz_pca_ind(pca_iris, repel = TRUE, col.ind = pca_df$Species)
non_scaled_iris = prcomp(iris_data, center = TRUE)
scaled_iris = prcomp(iris_data, center = TRUE, scale. = TRUE)
scaled_iris
non_scaled_iris
pca3d
summary(non_scaled_iris)
summary(scaled_iris)
summary(pca3d)
summary(non_scaled_iris)
summary(scaled_iris)
fviz_pca_ind(pca_iris, repel = TRUE, col.ind = pca_df$Species)
##### PRINCIPAL COMPONENT ANALYSIS (PCA) #####
### üìå INTRODUCTION
# PCA is a statistical technique used to reduce the dimensionality of a dataset
# while retaining as much variance (information) as possible.
#
# It transforms the original variables into a new set of variables ‚Äî called
# principal components ‚Äî that are uncorrelated and ordered by the amount of variance they explain.
# Why use PCA?
# - To summarize datasets with many correlated variables
# - To visualize complex data in 2D or 3D
# - To detect patterns, clusters, or anomalies
# Important: PCA works on numerical, scaled data.
############################################################
### üîç PART 1 ‚Äî PCA on Simulated Data
############################################################
# Simulate a 2D dataset with correlation
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100, sd = 0.5)
df <- data.frame(x = x, y = y)
# Plot original data
library(ggplot2)
ggplot(df, aes(x = x, y = y)) +
geom_point(alpha = 0.6) +
labs(title = "Original Correlated Data")
# Run PCA
pca_1 <- prcomp(df, center = TRUE, scale. = TRUE)
summary(pca_1)
## Importance of components:
##                          PC1     PC2
## Standard deviation     1.402 0.18464
## Proportion of Variance 0.983 0.01705
## Cumulative Proportion  0.983 1.00000
# Plot PCA-transformed data
df_pca <- as.data.frame(pca_1$x)
ggplot(df_pca, aes(x = PC1, y = PC2)) +
geom_point(alpha = 0.6, color = "blue") +
labs(title = "PCA: Rotated Axes (PC1 & PC2)")
# Scree plot: how much variance is explained?
var_explained <- pca_1$sdev^2 / sum(pca_1$sdev^2)
barplot(var_explained,
main = "Proportion of Variance Explained",
names.arg = paste0("PC", 1:length(var_explained)))
############################################################
### üîç PART 2 ‚Äî PCA on Real Data (iris)
############################################################
data(iris)
# Use only numeric columns
iris_data <- iris[, 1:4]
iris_scaled <- scale(iris_data)
# Run PCA
pca_iris <- prcomp(iris_scaled)
# Variance explained
summary(pca_iris)
## Importance of components:
##                           PC1    PC2     PC3     PC4
## Standard deviation     1.7084 0.9560 0.38309 0.14393
## Proportion of Variance 0.7296 0.2285 0.03669 0.00518
## Cumulative Proportion  0.7296 0.9581 0.99482 1.00000
# Combine with species info
pca_df <- as.data.frame(pca_iris$x)
pca_df$Species <- iris$Species
# Plot first two principal components, colored by species
ggplot(pca_df, aes(x = PC1, y = PC2, color = Species)) +
geom_point(size = 2, alpha = 0.7) +
labs(title = "PCA of Iris Dataset",
x = "Principal Component 1",
y = "Principal Component 2") +
theme_minimal()
# Visualize variable loadings (optional)
library(factoextra)
## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa
fviz_pca_var(pca_iris,
col.var = "contrib",
gradient.cols = c("white", "blue", "red"),
repel = TRUE)
#Unwritten rule: A model with more than 3 variables is bullshit.
############################################################
### üß† CLASS EXERCISES
############################################################
# üß™ EXERCISE 1 ‚Äî Simulated data ------------------------------------------
# Create a 3-variable dataset where all variables are highly correlated.
# Run PCA and plot PC1 vs PC2. Interpret.
#Simulate a 3D dataset.
x = rnorm(100, sd = 0.2)
y = 4 * x + rnorm(100, sd = 0.6)
z = 2 * y + rnorm(100, sd = 0.6)
data3d = data.frame(x = x, y = y, z = z)
cor(data3d)
ggplot() +
geom_point(data = data3d, aes(x = x, y = y, col = "red")) +
geom_point(data = data3d, aes(x = x, y = z, col = "blue")) +
geom_point(data = data3d, aes(x = y , y = z, col = "green"))
pca3d = prcomp(data3d, center = TRUE, scale. = TRUE)
summary(pca3d)
df_pca3d = as.data.frame(pca3d$x)
ggplot(data = df_pca3d, aes(x = PC1, y = PC2)) +
geom_point(alpha = 0.5, col = "red")
#We can see that the range of values that PC1 involves is higher than PC2 and much higher than PC3, which
#supports the proportion of variance described in the summary.
# üß™ EXERCISE 2 ‚Äî Iris PCA ------------------------------------------------
# Use the iris dataset and compute PCA.
# Plot PC1 vs PC3 instead of PC1 vs PC2. Does separation improve?
iris_data <- iris[, 1:4]
iris_scaled <- scale(iris_data)
# Run PCA
pca_iris <- prcomp(iris_scaled)
summary(pca_iris)
pca_df <- as.data.frame(pca_iris$x)
pca_df$Species <- iris$Species
ggplot(data = pca_df) +
geom_point(aes(x = PC1, y = PC2, col = "red"),size = 2, alpha = 0.7) +
geom_point(aes(x = PC1, y = PC3, col = "blue"),size = 2, alpha = 0.7) +
labs(title = "PCA of Iris Dataset")
#Yes the separation improve.
# üß™ EXERCISE 3 ‚Äî Variance explained --------------------------------------
# Compute the proportion of variance explained by each component.
# How many PCs are needed to explain at least 90% of the variance?
var_explained_iris = pca_iris$sdev**2/sum(pca_iris$sdev**2)
#We need at least 2 PCS, PC1 and PC2 to explain at least 90% of the variance.
# üß™ EXERCISE 4 ‚Äî Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
fviz_pca_ind(pca_iris, repel = TRUE, col.ind = pca_df$Species)
# üß™ EXERCISE 5 ‚Äî Standardization test ------------------------------------
# Run PCA on unscaled iris data. Compare results with the scaled version.
# Why is scaling important?
non_scaled_iris = prcomp(iris_data, center = TRUE)
scaled_iris = prcomp(iris_data, center = TRUE, scale. = TRUE)
summary(non_scaled_iris)
summary(scaled_iris)
#Scaling is important in order for us to really understand correctly which principal component explain
#better the data.
############################################################
# END OF LESSON
############################################################
# üß™ EXERCISE 4 ‚Äî Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
fviz_pca_ind(pca_iris, repel = TRUE, col.ind = pca_df$Species)
# üß™ EXERCISE 4 ‚Äî Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
fviz_pca_ind(pca_iris, repel = TRUE, col.ind = pca_df$Species)
seg.raw <- read.csv("http://goo.gl/qw303p")
head(seg.raw)
gc()
seg.raw <- read.csv("http://goo.gl/qw303p")
head(seg.raw)
seg.df  <- seg.raw[ , -7]     # remove the known segment assignments
head(seg.df)
#We create a simple function to look at mean values by group.
#(This is a placeholder for a more complex evaluation of an interpretable business outcome.)
seg.summ <- function(data, groups) {
aggregate(data, list(groups), function(x) mean(as.numeric(x)))
}
seg.summ(seg.df, seg.raw$Segment)
#dist() computes Euclidian distance:
sqrt(sum((c(1,2,3) - c(2,3,2))^2))
dist(rbind(c(1,2,3), c(2,3,2)))
#In case of mixed data types (e.g., continuous, binary, ordinal),
#dist() may not be appropriate because of the huge implied scale differences.
#daisy() is an alternative that automatically rescales.
library(cluster)
seg.dist <- daisy(seg.df)       # daisy works with mixed data types
install.packages("igraph")
library(igraph)
g1 <- graph(edges=c(1,2, 2,3, 3,1), n=3, directed=F)
g1
g2 <- graph( edges=c(1,2, 2,3, 3,1, 4,1, 10,1), n=10 , directed = F)
plot(g2)
g1 <- graph(edges=c(1,2, 2,3, 3,1), n=3, directed=F)
g2 <- graph( edges=c(1,2, 2,3, 3,1, 4,1, 10,1), n=10 , directed = F)
plot(g2)
g3 <- graph( c("John", "Jim", "Jim", "Jill", "Jill", "John")) # named vertices
# When the edge list has vertex names, the number of nodes is not needed
plot(g3)
g4 <- graph( c("John", "Jim", "Jim", "Jack", "Jim", "Jack", "John", "John"),
isolates=c("Jesse", "Janis", "Jennifer", "Justin") )
# In named graphs we can specify isolates by providing a list of their names.
plot(g4)
plot(g4, edge.arrow.size=.5, vertex.color="gold", vertex.size=15,
vertex.frame.color="gray", vertex.label.color="black",
vertex.label.cex=1.5, vertex.label.dist=2, edge.curved=0.2)
plot(g4, edge.arrow.size=.5, vertex.color=V(g4)$color, vertex.size=15,
vertex.frame.color="gray", vertex.label.color="black",
vertex.label.cex=1.5, vertex.label.dist=2, edge.curved=0.2)
g4 <- set_graph_attr(g4, "name", "Email Network")
g4 <- set_graph_attr(g4, "Collection Year", "2000")
graph_attr_names(g4)
## [1] "name"            "Collection Year"
g4 <- delete_graph_attr(g4, "Collection Year")
graph_attr(g4)
## $name
## [1] "Email Network"
# similarly we can use set_edge_attr() and set_vertex_attr()
g4 <- set_vertex_attr(g4,"color",value=c("skyblue", "skyblue", "skyblue", "skyblue",
"bisque", "bisque", "skyblue"))
plot(g4, edge.arrow.size=.5, vertex.color=V(g4)$color, vertex.size=15,
vertex.frame.color="gray", vertex.label.color="black",
vertex.label.cex=1.5, vertex.label.dist=2, edge.curved=0.2)
watts.strogatz.game()
watts.strogatz.game(100)
watts.strogatz.game(100, 100)
watts.strogatz.game(100, 100, 10)
watts.strogatz.game(100, 100, 10, p = 0.01)
watts.strogatz.game(10, 10, 10, p = 0.01)
watts.strogatz.game(2, 2, 3, p = 0.01)
a = watts.strogatz.game(2, 2, 3, p = 0.01)
plot(a)
a = watts.strogatz.game(2, 4, 4, p = 0.01)
plot(a)
setwd("~/Documentos/Uni/2nd Year 2nd Semester/Data Project")
setwd("~/Documents/uni/Data Project")
# setwd("~/Documentos/Uni/2nd Year 2nd Semester/Data Project")
library(dplyr)
library(purrr)
library(tidyr)
library(ggplot2)
library(igraph)
# Data Analysis -----------------------------------------------------------
#My question is: Is it possible that the returns of a commodity that in theory shouldn't
#impact on Microsoft, impact on the price of this company?
#First of all, we have the document all together. We know the first column is the date.
#The second is the stock.
#The next 5 are the ones in theory should be correlated. we call this the Impact group.
#GOLD, SILVER, PLATINUM, COPPER, PALADIUM.
#The others shouldn't be correlated, we will call the No Impact group.
#Brent Crude Oil, Natural Gas, Corn, Oats, Rice, Soybeans, Cocoa, Coffee, Cotton, Sugar.
stock = "MSFT"
returns = returns[complete.cases(returns), ] = returns = read.csv("returns_stocks_commodities.csv")
returns_no_date = returns[complete.cases(returns), ][-1]
returns_long = pivot_longer(returns, cols = -Date, names_to = "Commodity", values_to = "Return") %>%
filter(Commodity != stock) #Drop stock reference
returns_long_ref = returns_long %>% left_join(returns %>% select(Date, all_of(stock)), by = "Date") #Left join it.
#We want to check how is the shape of the data, we will do a scatter plot.
#We need to limit the y axis to the min, max Microsoft, to not get scaled.
min_msft = min(returns_long_ref[[stock]], na.rm = TRUE)
max_msft = max(returns_long_ref[[stock]], na.rm = TRUE)
ggplot(data = returns_long_ref, aes(x = Return, y = !!sym(stock), color = Commodity)) +
geom_point() +
facet_wrap(~ Commodity) +
scale_y_continuous(limits = c(min_msft, max_msft)) +
labs(title = "Microsoft Returns X Commodities Returns", x = "Commodity Returns", y = "Microsoft Returns",
caption = "(based on data from Yahoo Finance)")
#First of all, we want to understand each case, how does the stock behaves in correlation to the others.
correlation_table = data.frame(values = cor(returns_no_date, method = "spearman")[1, -1])
#First the Impact
correlation_table[1:6, ]
#We understand that Microsoft is pratically neutral correlated to gold. To the others it has a weak positive
#correlation.
#Now the No Impact
correlation_table[7: ncol(returns_no_date), ]
#We found that it has some positive and negative correlations in general.
#We now use a bar plot to plot all this correlation table with respect to MSFT.
#We will use ggplot library because it's better to design and control the variables.
labels = rownames(correlation_table)
ggplot(data = correlation_table, aes(x = labels, y = values, fill = values)) +
geom_col() +
scale_fill_gradient2(name = "Gradient",
high = "blue", low = "red", mid = "purple", midpoint = mean(correlation_table$values)) +
labs(title = "Microsoft Correlation Table", x = "Commodities", y = "Values",
caption = "(based on data from Yahoo Finance)")
#Now that we plotted this and we understand the differences between each commodity. They are not significant.
#We will now do a linear regression to understand how does each correlation behave.
#For this we will do a facetwrap in which we do a plot for each commodity.
ggplot(data = returns_long_ref, aes(x = Return, y = !!sym(stock))) +
facet_wrap(~ Commodity) +
geom_point() +
scale_y_continuous(limits = c(min_msft, max_msft)) +
geom_smooth(method = "lm", se = FALSE, col = "blue") +
labs(title = "MSFT Correlation Lines")
#Plotting these lines we understand that we cannot find a linear relationship between these commodities.
#Since, we are able to understand how these are under fitting the data.
#So, what we want is to understand if even in specific moments, this holds.
#We will have two measures of extremity. 2std from the mean and 3std from the mean.
#We can affirm that the first measure we are outside of 95% of the cases. The second 99.7% of the cases.
#However, to be able to affirm this, we need to check the histogram of each commodity, since this holds in
#a normal distribution.
returns_long = returns_long[!is.na(returns_long$Return), ]
#First we will calculate the mean and standard deviation of each commodity.
stats = returns_long %>%
group_by(Commodity) %>%
summarise(mean = mean(Return), sd = sd(Return))
#Then we will calculate a PDF for each commodity.
# Generate x-values covering the range of returns
x_range = seq(min(returns_long$Return), max(returns_long$Return), length.out = 100)
# Calculate normal curve points per commodity
norm_curves = stats %>%
group_by(Commodity) %>%
reframe(
x = x_range,
y = dnorm(x, mean = mean, sd = sd)  # Use group-specific mean/sd
)
# Plotting the histogram and normal distribution for each commodity
ggplot(data = returns_long, aes(x = Return, y = ..density.., fill = Commodity)) +
geom_histogram(bins = 50) +
geom_line(data = norm_curves, aes(x = x, y = y), color = "blue") +
facet_wrap(~ Commodity)
#Observing the graph, we understand that they don't follow normal distributions, even if it's similar.
#Therefore, we need to convert each commodity to a z-score and then get the dates when the values are extremities.
returns_long_z = returns_long %>%
left_join(stats, by = "Commodity") %>%
mutate(z_score = (Return - mean) / sd)
#Since Z-scores means how many standard deviations a value is from the mean, we can now filter the dates.
#We will filter dates that the z-score: |2| <= Z <= |3|, for 2std extremities and Z >= |3| for 3std extremities.
#One concern is that since we are getting the dates where these extremities happened, what occurs is that
#while one commodity is in an extremity, the others are not. This can lead to a bias in the correlation.
#Therefore, we have to calculate the correlation of one commodity at a time.
#We will get first the dates of the extremities of each commodity and then do the correlation.
extremities_by_commodity_2std = returns_long_z %>%
group_by(Commodity) %>%
filter(abs(z_score) >= 2 & abs(z_score) < 3) %>%
summarise(dates = list(unique(Date)))
extremities_by_commodity_3std = returns_long_z %>%
group_by(Commodity) %>%
filter(abs(z_score) >= 3) %>%
summarise(dates = list(unique(Date)))
#Now we will calculate the correlation for each commodity in the extremities.
# 1. Define a function to calculate correlation for a commodity's extreme dates
get_data <- function(comm, extreme_dates, returns_df, stock_col) {
comm_returns <- returns_df %>%
filter(Date %in% extreme_dates) %>%
pull(comm)
stock_returns <- returns_df %>%
filter(Date %in% extreme_dates) %>%
pull(stock_col)
data.frame(x = comm_returns, y = stock_returns)
}
cor_test_2std = extremities_by_commodity_2std %>%
mutate(data = map2(Commodity, dates, ~ get_data(.x, .y, returns, stock))) %>%
unnest(data) %>%
group_by(Commodity) %>%
summarise(cor_test = list(cor.test(x, y, method = "spearman")),
p_value = map_dbl(cor_test, ~ .x$p.value),
)
cor_test_3std = extremities_by_commodity_3std %>%
mutate(data = map2(Commodity, dates, ~ get_data(.x, .y, returns, stock))) %>%
unnest(data) %>%
group_by(Commodity) %>%
summarise(cor_test = list(cor.test(x, y, method = "spearman", conf.level = 0.95, exact = FALSE)),
p_value = map_dbl(cor_test, ~ .x$p.value),
ci_lower = map(cor_test, ~ .x$conf.int[1]),
ci_upper = map(cor_test, ~ .x$conf.int[2])
)
#The ones with values greater than 0.05 are not statistically significant.
cor_test_2std <- cor_test_2std %>%
mutate(Correlation = map_dbl(cor_test, ~ .x$estimate)) %>%
select(Commodity, Correlation, p_value)
cor_test_3std <- cor_test_3std %>%
mutate(Correlation = map_dbl(cor_test, ~ .x$estimate)) %>%
select(Commodity, Correlation, p_value)
#Now, we will plot the results.
ggplot(data = cor_test_2std, aes(x = Commodity, y = Correlation, fill = Correlation)) +
geom_col() +
scale_fill_gradient2(name = "Gradient",
high = "blue", low = "red", mid = "purple",
midpoint = mean(cor_test_2std$Correlation)) +
geom_text(aes(label = round(Correlation, 2)), vjust = -0.5, color = "black", size = 3) +
labs(title = "Microsoft Correlation Table Extremities 2std", x = "Commodities", y = "Values",
caption = "(based on data from Yahoo Finance)")
ggplot(data = cor_test_3std, aes(x = Commodity, y = Correlation, fill = Correlation)) +
geom_col() +
scale_fill_gradient2(name = "Gradient",
high = "blue", low = "red", mid = "purple",
midpoint = mean(cor_test_3std$Correlation)) +
geom_text(aes(label = round(Correlation, 2)), vjust = -0.5, color = "black", size = 3) +
labs(title = "Microsoft Correlation Table Extremities 3std", x = "Commodities", y = "Values",
caption = "(based on data from Yahoo Finance)")
#We see that the intensity of correlation is higher in both plots, specially, in the 3std plot.
#We observe that some commodities have one behaviour on the 2std and another on the 3std, as KC.F(Coffe),
#NG.F(Natural Gas), CC.F(Cocoa), CT.F(Cotton), GC.F(Gold), HG.F(Copper), SB.F(Soybeans), ZO.F(Oats).
#Now, we see that Microsoft has a correlation with commodities that shouldn't impact it.
#In the end, we want to see if we can assume this correlation is statistically significant.
#We will do a correlation test for each commodity in the extremities.
# Filter significant commodities
sig_results <- cor_test_2std %>%
filter(p_value < 0.06)  # NG.F included for illustration
#Now, we will plot the same scatterplot with the linear regression line to see if we can find a more linear relation.
#We will use a similar function, to create a dataframe where we have the stock returns and commodity returns, for
#each one of the commodity.
# 2. Apply to all commodities (vectorized)
returns_specific_2std = extremities_by_commodity_2std %>%
mutate(data = map2(Commodity, dates, ~ get_data(.x, .y, returns, stock))) %>%
unnest(data)
head(returns_specific_2std)
returns_specific_3std = extremities_by_commodity_3std %>%
mutate(data = map2(Commodity, dates, ~ get_data(.x, .y, returns, stock))) %>%
unnest(data)
# 3. Create the scatterplot
ggplot(data = returns_specific_2std, aes(x = x, y = y, color = Commodity)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, col = "blue") +
facet_wrap(~ Commodity) +
labs(title = "Microsoft Returns X Commodities Returns Extremities 2std",
x = "Commodity Returns", y = "Microsoft Returns",
caption = "(based on data from Yahoo Finance)")
#We still observe that the linear regression line is not fitting the data well, therefore, is not a linear relationship.
ggplot(data = returns_specific_3std, aes(x = x, y = y, color = Commodity)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, col = "blue") +
facet_wrap(~ Commodity) +
labs(title = "Microsoft Returns X Commodities Returns Extremities 3std",
x = "Commodity Returns", y = "Microsoft Returns",
caption = "(based on data from Yahoo Finance)")
#On this one, there is a better linear relationship, however, it still really poor.
#Conclusion: There isn't a linear relationship between the commodities and MSFT.
#"During extreme events, Sugar (SB.F) shows a weak inverse correlation with MSFT (œÅ = -0.19, p = 0.047),
#while Silver (SI.F) and Natural Gas (NG.F) exhibit weak positive links
#(œÅ = 0.16, p = 0.049 and œÅ = 0.17, p = 0.055, respectively).
#Other commodities show no statistically significant relationships."
# Network Analysis --------------------------------------------------------
#We will now do a network analysis to understand how the industries of each commodity had an
#indirect impact on Microsoft, using the correlations as the weights.
#We will also see the links between the major commodities country producer and the stock.
#We will use the correlation table we created before, to create a network.
#We will use the igraph library to create the network.
correlation_network = cor_test_3std %>%
select(Commodity, Correlation) %>%
mutate(Stock = stock)
#Create a dataset for each commodity, where there are the industries related by each commodity.
#"BZ.F" "CC.F" "CT.F" "GC.F" "HG.F" "KC.F" "NG.F" "PA.F" "PL.F" "SB.F" "SI.F" "ZC.F" "ZO.F" "ZR.F" "ZS.F"
commodity_industries = data.frame(
Commodity = c("BZ.F", "CC.F", "CT.F", "GC.F", "HG.F",
"KC.F", "NG.F", "PA.F", "PL.F", "SB.F",
"SI.F", "ZC.F", "ZO.F", "ZR.F", "ZS.F"),
Industry = c("Transportation, Petrochemicals, Energy",
"Chocolate, Cosmetics, Food", "Textiles, Medical, Apparel",
"Mining, Jewelry, Electronics, Finance", "Mining, Construction, Electronics, EVs",
"Beverages, Food Service", "Electricity, Heating, Manufacturing, Transport",
"Mining, Automotive Catalysts, Electronics", "Mining, Automotive, Jewelry, Chemicals",
"Food, Biofuel, Beverages", "Mining, Solar Panels, Electronics, Jewelry",
"Animal Feed, Biofuel, Food", "Animal Feed, Food Products",
"Food, Beverages, Biofuel", "Animal Feed, Biofuel, Food"))
#Now we will join to this dataset the correlation network dataset.
commodity_industries = commodity_industries %>%
left_join(correlation_network, by = "Commodity")
#Now we will change this dataset in order to have only one industry per row.
commodity_industries_long = commodity_industries %>%
group_by(Commodity) %>%
separate_rows(Industry, sep = ", ") %>%
mutate(Industry = trimws(Industry))  # Remove any leading/trailing whitespace
#Now we will create a network graph using the igraph library.
#Prepare edges
edges = commodity_industries %>%
select(from = Commodity, to = Stock, weights = Correlation)
#Prepare the edges from Commodity to Industry with a symbolic weight in the weights
edges_industry = commodity_industries_long %>%
select(from = Commodity, to = Industry, weights = Correlation)
edges_combined = bind_rows(edges, edges_industry)
#Now we will prepare the vertices
vertices = data.frame(name = unique(c(edges$from, edges$to, edges_industry$to)),
type = c(rep("Commodity", length(unique(edges$from))),
rep("Stock", length(unique(edges$to))),
rep("Industry", length(unique(edges_industry$to)))))
#Create the graph
g = graph_from_data_frame(d = edges_combined, vertices = vertices, directed = FALSE)
#Plot the graph
plot(g,
layout = layout_with_kk(g, kkconst = vcount(g)**39),  # Adjust repulsion for better spacing
vertex.label = V(g)$name,
vertex.label.cex = 0.6,
vertex.label.color = "black",  # Darker for readability
vertex.size = sqrt(degree(g)) * 5,
vertex.color = ifelse(V(g)$type == "Commodity", "#2E8B57",
ifelse(V(g)$type == "Stock", "#4169E1", "#B22222")),
edge.width = E(g)$weights * E(g)$weights* 40,  # Less aggressive scaling
edge.color = ifelse(E(g)$weights > 0, "#4682B4", "#FF6347"),
main = "Network of Commodities, Industries and Microsoft Stock",
margin = c(0, 0, 0, 0))  # Maximize plot area
