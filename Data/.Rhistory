# Run PCA
pca_1 <- prcomp(df, center = TRUE, scale. = TRUE)
summary(pca_1)
## Importance of components:
##                          PC1     PC2
## Standard deviation     1.402 0.18464
## Proportion of Variance 0.983 0.01705
## Cumulative Proportion  0.983 1.00000
# Plot PCA-transformed data
df_pca <- as.data.frame(pca_1$x)
ggplot(df_pca, aes(x = PC1, y = PC2)) +
geom_point(alpha = 0.6, color = "blue") +
labs(title = "PCA: Rotated Axes (PC1 & PC2)")
# Scree plot: how much variance is explained?
var_explained <- pca_1$sdev^2 / sum(pca_1$sdev^2)
barplot(var_explained,
main = "Proportion of Variance Explained",
names.arg = paste0("PC", 1:length(var_explained)))
############################################################
### ðŸ” PART 2 â€” PCA on Real Data (iris)
############################################################
data(iris)
# Use only numeric columns
iris_data <- iris[, 1:4]
iris_scaled <- scale(iris_data)
# Run PCA
pca_iris <- prcomp(iris_scaled)
# Variance explained
summary(pca_iris)
## Importance of components:
##                           PC1    PC2     PC3     PC4
## Standard deviation     1.7084 0.9560 0.38309 0.14393
## Proportion of Variance 0.7296 0.2285 0.03669 0.00518
## Cumulative Proportion  0.7296 0.9581 0.99482 1.00000
# Combine with species info
pca_df <- as.data.frame(pca_iris$x)
pca_df$Species <- iris$Species
# Plot first two principal components, colored by species
ggplot(pca_df, aes(x = PC1, y = PC2, color = Species)) +
geom_point(size = 2, alpha = 0.7) +
labs(title = "PCA of Iris Dataset",
x = "Principal Component 1",
y = "Principal Component 2") +
theme_minimal()
# Visualize variable loadings (optional)
library(factoextra)
## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa
fviz_pca_var(pca_iris,
col.var = "contrib",
gradient.cols = c("white", "blue", "red"),
repel = TRUE)
#Unwritten rule: A model with more than 3 variables is bullshit.
############################################################
### ðŸ§  CLASS EXERCISES
############################################################
# ðŸ§ª EXERCISE 1 â€” Simulated data ------------------------------------------
# Create a 3-variable dataset where all variables are highly correlated.
# Run PCA and plot PC1 vs PC2. Interpret.
#Simulate a 3D dataset.
x = rnorm(100, sd = 0.2)
y = 4 * x + rnorm(100, sd = 0.6)
z = 2 * y + rnorm(100, sd = 0.6)
data3d = data.frame(x = x, y = y, z = z)
cor(data3d)
ggplot() +
geom_point(data = data3d, aes(x = x, y = y, col = "red")) +
geom_point(data = data3d, aes(x = x, y = z, col = "blue")) +
geom_point(data = data3d, aes(x = y , y = z, col = "green"))
pca3d = prcomp(data3d, center = TRUE, scale. = TRUE)
summary(pca3d)
df_pca3d = as.data.frame(pca3d$x)
ggplot(data = df_pca3d, aes(x = PC1, y = PC2)) +
geom_point(alpha = 0.5, col = "red")
#We can see that the range of values that PC1 involves is higher than PC2 and much higher than PC3, which
#supports the proportion of variance described in the summary.
# ðŸ§ª EXERCISE 2 â€” Iris PCA ------------------------------------------------
# Use the iris dataset and compute PCA.
# Plot PC1 vs PC3 instead of PC1 vs PC2. Does separation improve?
iris_data <- iris[, 1:4]
iris_scaled <- scale(iris_data)
# Run PCA
pca_iris <- prcomp(iris_scaled)
summary(pca_iris)
pca_df <- as.data.frame(pca_iris$x)
pca_df$Species <- iris$Species
ggplot(data = pca_df) +
geom_point(aes(x = PC1, y = PC2, col = "red"),size = 2, alpha = 0.7) +
geom_point(aes(x = PC1, y = PC3, col = "blue"),size = 2, alpha = 0.7) +
labs(title = "PCA of Iris Dataset")
#Yes the separation improve.
# ðŸ§ª EXERCISE 3 â€” Variance explained --------------------------------------
# Compute the proportion of variance explained by each component.
# How many PCs are needed to explain at least 90% of the variance?
var_explained_iris = pca_iris$sdev**2/sum(pca_iris$sdev**2)
#We need at least 2 PCS, PC1 and PC2 to explain at least 90% of the variance.
# ðŸ§ª EXERCISE 4 â€” Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
fviz_pca_ind(pca_iris, repel = TRUE, col.ind = pca_df$Species)
# ðŸ§ª EXERCISE 5 â€” Standardization test ------------------------------------
# Run PCA on unscaled iris data. Compare results with the scaled version.
# Why is scaling important?
non_scaled_iris = prcomp(iris_data, center = TRUE)
scaled_iris = prcomp(iris_data, center = TRUE, scale. = TRUE)
summary(non_scaled_iris)
summary(scaled_iris)
#Scaling is important in order for us to really understand correctly which principal component explain
#better the data.
############################################################
# END OF LESSON
############################################################
# ðŸ§ª EXERCISE 4 â€” Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
fviz_pca_ind(pca_iris, repel = TRUE, col.ind = pca_df$Species)
# ðŸ§ª EXERCISE 4 â€” Visualize structure -------------------------------------
# Use the `fviz_pca_ind()` function (from factoextra) to show clustering by species.
fviz_pca_ind(pca_iris, repel = TRUE, col.ind = pca_df$Species)
seg.raw <- read.csv("http://goo.gl/qw303p")
head(seg.raw)
gc()
seg.raw <- read.csv("http://goo.gl/qw303p")
head(seg.raw)
seg.df  <- seg.raw[ , -7]     # remove the known segment assignments
head(seg.df)
#We create a simple function to look at mean values by group.
#(This is a placeholder for a more complex evaluation of an interpretable business outcome.)
seg.summ <- function(data, groups) {
aggregate(data, list(groups), function(x) mean(as.numeric(x)))
}
seg.summ(seg.df, seg.raw$Segment)
#dist() computes Euclidian distance:
sqrt(sum((c(1,2,3) - c(2,3,2))^2))
dist(rbind(c(1,2,3), c(2,3,2)))
#In case of mixed data types (e.g., continuous, binary, ordinal),
#dist() may not be appropriate because of the huge implied scale differences.
#daisy() is an alternative that automatically rescales.
library(cluster)
seg.dist <- daisy(seg.df)       # daisy works with mixed data types
install.packages("igraph")
library(igraph)
g1 <- graph(edges=c(1,2, 2,3, 3,1), n=3, directed=F)
g1
g2 <- graph( edges=c(1,2, 2,3, 3,1, 4,1, 10,1), n=10 , directed = F)
plot(g2)
g1 <- graph(edges=c(1,2, 2,3, 3,1), n=3, directed=F)
g2 <- graph( edges=c(1,2, 2,3, 3,1, 4,1, 10,1), n=10 , directed = F)
plot(g2)
g3 <- graph( c("John", "Jim", "Jim", "Jill", "Jill", "John")) # named vertices
# When the edge list has vertex names, the number of nodes is not needed
plot(g3)
g4 <- graph( c("John", "Jim", "Jim", "Jack", "Jim", "Jack", "John", "John"),
isolates=c("Jesse", "Janis", "Jennifer", "Justin") )
# In named graphs we can specify isolates by providing a list of their names.
plot(g4)
plot(g4, edge.arrow.size=.5, vertex.color="gold", vertex.size=15,
vertex.frame.color="gray", vertex.label.color="black",
vertex.label.cex=1.5, vertex.label.dist=2, edge.curved=0.2)
plot(g4, edge.arrow.size=.5, vertex.color=V(g4)$color, vertex.size=15,
vertex.frame.color="gray", vertex.label.color="black",
vertex.label.cex=1.5, vertex.label.dist=2, edge.curved=0.2)
g4 <- set_graph_attr(g4, "name", "Email Network")
g4 <- set_graph_attr(g4, "Collection Year", "2000")
graph_attr_names(g4)
## [1] "name"            "Collection Year"
g4 <- delete_graph_attr(g4, "Collection Year")
graph_attr(g4)
## $name
## [1] "Email Network"
# similarly we can use set_edge_attr() and set_vertex_attr()
g4 <- set_vertex_attr(g4,"color",value=c("skyblue", "skyblue", "skyblue", "skyblue",
"bisque", "bisque", "skyblue"))
plot(g4, edge.arrow.size=.5, vertex.color=V(g4)$color, vertex.size=15,
vertex.frame.color="gray", vertex.label.color="black",
vertex.label.cex=1.5, vertex.label.dist=2, edge.curved=0.2)
watts.strogatz.game()
watts.strogatz.game(100)
watts.strogatz.game(100, 100)
watts.strogatz.game(100, 100, 10)
watts.strogatz.game(100, 100, 10, p = 0.01)
watts.strogatz.game(10, 10, 10, p = 0.01)
watts.strogatz.game(2, 2, 3, p = 0.01)
a = watts.strogatz.game(2, 2, 3, p = 0.01)
plot(a)
a = watts.strogatz.game(2, 4, 4, p = 0.01)
plot(a)
wd = "~/Documents/uni/Data"
setwd(wd)
library(igraph)
nodes <- read.csv("Dataset1-Media-Example-NODES.csv", header=T, as.is=T)
links <- read.csv("Dataset1-Media-Example-EDGES.csv", header=T, as.is=T)
net <- graph_from_data_frame(d=links, vertices=nodes, directed=T)
# Density: The proportion of present edges from all possible ties.
edge_density(net, loops=F)
# [1] 0.1911765
ecount(net)/(vcount(net)*(vcount(net)-1)) #for a directed network
# [1] 0.1911765
# Reciprocity: The proportion of reciprocated ties (for a directed network).
reciprocity(net)
# [1] 0.3921569
dyad_census(net) # Mutual, asymmetric, and null node pairs
# $mut
# # [1] 10
#
# $asym
# # [1] 31
#
# $null
# [1] 95
2*dyad_census(net)$mut/ecount(net) # Calculating reciprocity
# [1] 0.3846154
#
# Transitivity (or Clustering Coefficient)
#
# global: ratio of triangles (direction disregarded) to connected triples
# local: ratio of triangles to connected triples each vertex is part of
transitivity(net, type="global")  # net is treated as an undirected network
# [1] 0.372549
transitivity(as.undirected(net, mode="collapse")) # same as above
# [1] 0.372549
transitivity(net, type="local")
# [1] 0.13333333 0.28571429 0.11538462 0.19444444 0.50000000 0.14285714
# [7] 0.20000000 0.06666667 0.20000000 0.30000000 0.33333333 0.20000000
# [13] 0.16666667 0.16666667 0.20000000 0.33333333 0.20000000
# Triad types (per Davis & Leinhardt):
# 003  A, B, C, empty triad.
# 012  A->B, C
# 102  A<->B, C
# 021D A<-B->C
# 021U A->B<-C
# 021C A->B->C
# 111D A<->B<-C
# 111U A<->B->C
# 030T A->B<-C, A->C
# 030C A<-B<-C, A->C.
# 201  A<->B<->C.
# 120D A<-B->C, A<->C.
# 120U A->B<-C, A<->C.
# 120C A->B->C, A<->C.
# 210  A->B<->C, A<->C.
# 300  A<->B<->C, A<->C, completely connected.
triad_census(net) # for directed networks
# [1] 244 231  90  13  11  27  15  22   4   1   8   4   4   3   3   0
# Network Descriptives
# Diameter (longest geodesic distance)
#Note that edge weights are used by default, unless set to NA.
diameter(net, directed=F, weights=NA)
# [1] 4
diameter(net, directed=F)
# [1] 28
diam <- get_diameter(net, directed=T)
diam
# + 7/17 vertices, named, from b8afdea:
# [1] s12 s06 s17 s04 s03 s08 s07
# Network Descriptives
# Diameter (longest geodesic distance)
# Color nodes along the diameter:
vcol <- rep("gray40", vcount(net))
vcol[diam] <- "gold"
ecol <- rep("gray80", ecount(net))
ecol[E(net, path=diam)] <- "orange"
# E(net, path=diam) finds edges along a path, here 'diam'
plot(net, vertex.color=vcol, edge.color=ecol, edge.arrow.mode=0)
# plot of chunk unnamed-chunk-6
# Network Descriptives
# Node degrees: 'degree' -has a mode of 'in' for in-degree,
# 'out' for out-degree,
# and 'all' or 'total' for total degree.
deg <- degree(net, mode="all")
plot(net, vertex.size=deg*3)
# plot of chunk unnamed-chunk-7
# EXERCISES ---------------------------------------------------------------
# 1. Import a directed network from a CSV file.
# Use graph_from_data_frame() to create a graph object.
# o Report: number of nodes, number of edges, edge density.
dir_net = graph_from_data_frame(d = links, vertices = nodes, directed = T)
vcount(dir_net)
ecount(dir_net)
edge_density(dir_net)
# 2. Compute in-degree and out-degree for each node.
# o Visualize both distributions with histograms.
# o Are hubs present? Comment on the shape.
in_degree = degree(dir_net, mode = "in")
out_degree = degree(dir_net, mode = "out")
barplot(in_degree)
barplot(out_degree)
#We can say that there are some hubs present, if we consider than in both there are some nodes who
#have more edges. However, there is not a huge discrepancy, since the distribution ressembles a little
#the uniform one.
# 3. Identify the top-5 nodes with highest out-degree.
# o What might this indicate in a social or information network?
View(out_degree)
out = data.frame(data = out_degree, stringsAsFactors = TRUE)
out = out[order(out$data, decreasing = TRUE),, drop = FALSE ]
head(out)
# s03    7
# s01    5
# s04    5
# s02    4
# s05    4
#We understand that these 5 nodes are the most influential.
#   4. Calculate the networkâ€™s reciprocity using reciprocity(g)
# o Is the network highly bidirectional or asymmetric?
#   o What would this imply in terms of interaction dynamics?
reciprocity(dir_net)
#We understand that the network is asymmetric since only 39% of the link are reciprocated.
#Only 39% of the vertices interact also with the vertices that originated the link they are receiving.
#   5. Perform a dyad census (dyad_census(g)).
# o What proportion of dyads are mutual?
#   o Compare this with an ErdÅ‘sâ€“RÃ©nyi random graph of the same size.
census = dyad_census(dir_net)
census$mut/sum(census$mut, census$asym, census$null)
sum(census$mut, census$asym) / sum(census$mut, census$asym, census$null)
#Only 7,35% are mutual.
erdos = sample_gnp(n = 17, p = 0.2794, directed = TRUE)
census = dyad_census(erdos)
census$mut/sum(census$mut, census$asym, census$null)
sum(census$mut, census$asym) / sum(census$mut, census$asym, census$null)
#We observe we have more less mutuality, but we have more connections between two arbitrary nodes.
# 6. Run a triad census (triad_census(g)).
# o Which triad types are most frequent?
#   o What do they suggest about hierarchy or closure?
triad_census(dir_net)
#We empty graph and the graph with only a single directed edge are more frequent, we have also a good
#amount of a mutual connection between two vertices.
#We understand that we do not have a good closure centrality and also we don't have a strong hierarchy.
#   7. Compute the local clustering coefficient for all nodes.
# o Use transitivity(g, type="local").
# o Plot the distribution and comment on network cohesion.
t = transitivity(dir_net, type = "local")
barplot(t)
#We understand that almost uniformly this network is not so cohesive.
# 8. Remove all nodes with zero out-degree.
# o What changes occur in triad count and average clustering?
out_ns = data.frame(data = out_degree)
nodes = nodes[which(out_ns != 0), ]
zero_node = nodes[which(out_ns == 0), 1]
links = links[links$to != zero_node, ]
no_zero = graph_from_data_frame(d = links, vertices = nodes, directed = TRUE)
count_triangles(dir_net)
degree(dir_net)
length(count_triangles(dir_net))
degree - 1
airports = read.csv("airports.csv", header = T, as.is= T)
routes = read.csv("routes.csv", header= T, as.is = T)
planes = graph_from_data_frame(d = routes, vertices = airports, directed = TRUE)
closeness(planes, mode ="all", weights = NA)
planes = graph_from_data_frame(d = routes, vertices = airports, directed = TRUE)
View(routes)
View(airports)
closeness(dir_net, mode ="all", weights = NA)
close
close = closeness(dir_net, mode ="all", weights = NA)
close
order(close, decreasing = TRUE)
order = order(close, decreasing = TRUE)
head(order)
betweenness(dir_net, directed = T, weights= NA)
bet = order(between, decreasing = TRUE)
between = betweenness(dir_net, directed = T, weights= NA)
bet = order(between, decreasing = TRUE)
head(bet)
nodes[which(out_ns == 0), 1]
three = bet[1:3]
bet[1:3]
View(links)
View(links)
ids = nodes[three, 1]
nodes[three, 1]
nodes <- read.csv("Dataset1-Media-Example-NODES.csv", header=T, as.is=T)
ids = nodes[three, 1]
ids
new_links = links[links$from & links$to != ids, ]
new_links = links[links$from != ids $ links$to != ids, ]
new_links = links[links$from != ids & links$to != ids, ]
new_links = links[links$from != ids & links$to != ids, ]
new_links = links[links$from != ids, ]
new_links = links[links$from != ids, ]
new_nodes = nodes[nodes$id != ids, ]
new_nodes = nodes[which(nodes$id != ids), ]
new_nodes = nodes[which(nodes$id != ids), ]
ids = nodes[three, 1]
new_nodes = nodes[nodes$id != three, ]
ids
new_links = links[!links$from %in% ids & !link$to %in% ids, ]
new_links = links[!links$from %in% ids & !links$to %in% ids, ]
new_nodes = nodes[!nodes$id %in% ids, ]
new_nodes
new_links
three = bet[1:3]
ids = nodes[three, 1]
new_nodes = nodes[!nodes$id %in% ids, ]
new_links = links[!links$from %in% ids & !links$to %in% ids, ]
new_graph = graph_from_data_frame(d = new_links, vertices = new_nodes, directed = TRUE)
old_d = diameter(dir_net)
new_d= diameter(new_graph)
old_dis = distances(dir_net, weights = NA)
new_dis = distances(new_graph, weights = NA)
#Diameters
old_d
new_d
#Distances
old_dis
new_dis
degree = degree(dir_net)
closeness = closeness(dir_net, weights= NA)
betwenn = betweenness(dir_net, weights = NA)
pagerank = page_rank(dir_net, weights = NA)
degree
closeness
betwenn
pagerank
dir_net = graph_from_data_frame(d = links, vertices = nodes, directed = T)
cor(pagerank)
rank = cor(x = pagerank$vector)
rank = cor(x = matrix(pagerank$vector))
rank
rank = cor(matrix(pagerank))
matrix(pagerank$vector)
rank = cor(matrix(pagerank$vector))
rank
rank = cor(x = matrix(pagerank$vector), y = matrix(pagerank$vector))
rank
rank = cor(x = pagerank$vector, y = pagerank$vector)
rank
rank = cor(x = pagerank$vector, y = degree)
rank
rank = cor(x = pagerank$vector, y = degree, method = "spearman")
rank
correlation
cor(iris[1:4])
rank = cor(x = c(degree, closeness, between, pagerank), method = "spearman")
degree
closeness
closeness(dir_net, weights= NA)
new_nod = nodes[which(out_ns != 0), ]
nodes <- read.csv("Dataset1-Media-Example-NODES.csv", header=T, as.is=T)
dir_net = graph_from_data_frame(d = links, vertices = nodes, directed = T)
degree = degree(dir_net)
degree(dir_net)
closeness(dir_net, weights= NA)
betweenness(dir_net, weights = NA)
nodes <- read.csv("Dataset1-Media-Example-NODES.csv", header=T, as.is=T)
links <- read.csv("Dataset1-Media-Example-EDGES.csv", header=T, as.is=T)
dir_net = graph_from_data_frame(d = links, vertices = nodes, directed = T)
closeness(dir_net, weights= NA)
betweenness(dir_net, weights = NA)
rankings = data.frame(degree = degree, closeness = closeness, betweenness = betwenn, pagerank = pagerank$vector)
rankings
rank = cor(rankings , method = "spearman")
rank
close = closeness(dir_net, mode ="all", weights = NA)
close
closeness = closeness(dir_net, mode = "all", weights= NA)
closeness
rankings = data.frame(degree = degree, closeness = closeness, betweenness = betwenn, pagerank = pagerank$vector)
rank = cor(rankings , method = "spearman")
rank
rankings
eigen = eigen_centrality(dir_net, directed = T, weights = NA)
eigen
plot(x = eigen$vector, y =pagerank$vector)
plot(dir_net, vertex.size = pagerank$vector, vertex.color = between)
plot.igraph(dir_net, vertex.size = pagerank$vector, vertex.color = between)
plot.igraph(dir_net, mark.expand = pagerank$vector, mark.col = between)
# Color nodes along the diameter:
vcol <- rep("gray40", vcount(net))
vcol[diam] <- "gold"
ecol <- rep("gray80", ecount(net))
ecol[E(net, path=diam)] <- "orange"
# E(net, path=diam) finds edges along a path, here 'diam'
plot(net, vertex.color=vcol, edge.color=ecol, edge.arrow.mode=0)
deg <- degree(net, mode="all")
plot(net, vertex.size=deg*3)
plot(net, vertex.size = pagerank$vector, vertex.color = between)
pagerank$vector
deg*3
plot(net, vertex.size = pagerank$vector*100, vertex.color = between)
plot(net, vertex.size = pagerank$vector*1000, vertex.color = between)
plot(net, vertex.size = pagerank$vector*500, vertex.color = between*500)
plot(net, vertex.size = pagerank$vector*500, vertex.color = between)
links
length = length(links)
length
length = length(links[, 1])
length
links[indices_to_replace, 3] = rnorm(round(mean(links$weight)))
length = length(links[, 1])
n = round(0.2 * length)
indices_to_replace = sample(length, n)
links[indices_to_replace, 3] = rnorm(round(mean(links$weight)))
rnorm(round(mean(links$weight)))
links[indices_to_replace, 3] = rnorm(n)
links[indices_to_replace, 3]
links
new_link = links[indices_to_replace, 3] = rnorm(n, mean = mean(links$weight), sd = 3)
new_link
length = length(links[, 1])
n = round(0.2 * length)
indices_to_replace = sample(length, n)
new_link = links[indices_to_replace, 3] = rnorm(n, mean = mean(links$weight), sd = 3)
new = graph_from_data_frame(d = new_link, vertices = nodes)
new = graph_from_data_frame(d = links, vertices = nodes)
newb = betweenness(new)
pagenew = page_rank(new)
newb, pagenew
pagenew
newb
diffb = newb - between
diffp = pagerank - pagenew
diffp = pagerank - pagenew
diffp = pagerank$vector - pagenew$vector
diffp
diffb
